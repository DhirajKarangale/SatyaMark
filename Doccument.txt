I am building a microservice-based system called SatyaMark.
Here is the complete architecture and implementation plan.
Please read this entire description and use it as context for all future answers.

ğŸš€ 1. High-Level Flow

My system processes long-running AI tasks (3â€“8 minutes).
The exact flow is:

Frontend â†’ API Gateway â†’ Auth Service + Rate-Limit Service â†’ Backend Service
â†’ AI Service (enqueue job) â†’ Redis Streams â†’ Worker(s)
â†’ Result Receiver â†’ DB + Cache â†’ WebSocket â†’ Frontend

ğŸ§© 2. Services and Their Responsibilities
Frontend (React)

Sends requests with JWT.

Listens to WebSocket for final results.

Every request includes a taskId so results can be matched.

API Gateway (Node.js)

Entry point for all requests.

Validates JWT by calling Auth Service.

Performs centralized rate-limiting by calling Rate-Limit Service.

Forwards allowed requests to Backend Service.

Auth Service (Node.js)

Issues and validates JWT tokens.

Stores user accounts.

No AI logic.

Rate-Limit Service (Node.js)

Enforces per-user & per-endpoint rate limits.

Uses Redis (token bucket).

API Gateway queries this before forwarding anything.

Backend Service (Node.js)

This service contains the â€œbusiness logic.â€

Responsibilities:

Read userId from JWT.

Compute:

textHash

summaryHash

Check Redis cache:

If HIT â†’ return cached result.

If MISS â†’ continue.

Generate taskId.

Store initial task record in DB.

Call AI Service to enqueue the job.

Respond to frontend: { taskId, queued: true }.

AI Service (FastAPI / Python, CPU only)

Does NOT run AI models.

Only enqueues jobs into Redis Streams.

Receives from Backend:

taskId

userId

payload

textHash

summaryHash

job_token

callback_url

Adds job to Redis Stream (XADD ai:jobs).

Redis Streams (Upstash Redis)

Used as the reliable distributed queue.

Supports consumer-groups.

Ensures only one worker gets each job.

Unprocessed jobs remain in pending list.

Workers can claim abandoned jobs (XCLAIM).

Worker Services (Python, GPU)

Multiple independent worker containers.

Consume Redis Stream jobs (XREADGROUP).

Run actual AI model inference.

Produce:

mark

reason

confidence

POST final result to Result Receiver using the callback_url.

Include job_token or HMAC for security.

Workers are stateless and fully scalable.

Result Receiver (Node.js)

Receives worker callback (/ai-callback).

Validates authenticity (HMAC).

Stores result into PostgreSQL (ai_results).

Writes result into Redis Cache (cache:result:<hash>).

Notifies WebSocket service that user has a new result.

WebSocket Service (Node.js)

Maintains connected clients.

Maps userId â†’ socketIds stored in Redis.

Pushes final results to the correct user.

Frontend receives a JSON message that includes taskId.

PostgreSQL (Neon.tech)

Table: ai_results
Columns:

id (taskId)

user_id

text_hash

summary_hash

mark

reason

confidence

timestamps

Stores final verified results.

ğŸ” 3. Detailed End-to-End Flow
Step 1 â€” Frontend â†’ API Gateway

Sends request with JWT + payload.

Step 2 â€” API Gateway

Validates JWT via Auth Service.

Checks quota via Rate-Limit Service.

Forwards to Backend.

Step 3 â€” Backend

Computes hashes.

Checks Redis cache.

If new:

Creates taskId

Writes task record

Calls AI Service

Step 4 â€” AI Service

Enqueues job in Redis Streams.

Step 5 â€” Workers

Read job from Redis.

Run AI inference.

POST result â†’ Result Receiver.

Step 6 â€” Result Receiver

Validates callback.

Writes result to DB + Redis cache.

Emits event to WebSocket service.

Step 7 â€” WebSocket Service

Pushes result to all connected sockets of that user.

Step 8 â€” Frontend

Receives result + taskId and updates UI.

ğŸ“¦ 4. Cache Keys
cache:result:<textHash>
cache:result:<summaryHash>
ws:user:<userId>
stream:ai:jobs

ğŸ§± 5. Queue Behavior

Redis Streams handles distributing jobs.

Workers join a consumer group.

Supports retry, pending lists, claiming stale jobs.

Ideal for long-running (3â€“8 min) tasks.

ğŸ’¾ 6. Scaling Notes

Add more workers for faster processing.

WebSocket service uses Redis to scale horizontally.

API Gateway can scale statelessly.

Result Receiver is lightweight and can scale too.

ğŸ”§ 7. Development / Free Hosting Plan

Node services â†’ Render / Railway (free)

AI API â†’ Render / Deta Space (free)

DB â†’ Neon.tech (free Postgres)

Redis â†’ Upstash (free Redis)

Workers â†’ local GPU OR HF Spaces (free dev GPU)

Frontend â†’ Vercel

ğŸ“Œ 8. IMPORTANT: What I Expect From The AI

When I ask questions, you should:

Use this architecture as context.

Refer to the service names exactly as described.

Suggest improvements compatible with my architecture.

Provide code examples in Node.js (for services) and Python (for AI).

Maintain IO patterns (taskId, callback_url, Redis Streams).

Never propose polling.

Always preserve the same flow:
Backend â†’ AI Service â†’ Queue â†’ Worker â†’ Result Receiver â†’ WebSocket.